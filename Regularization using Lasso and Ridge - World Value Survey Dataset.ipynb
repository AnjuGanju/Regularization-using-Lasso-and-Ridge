{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Condition numbers and regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This individual problem set has three aims:\n",
    "learn to fi􏰀nd problematic data (multicollinearity) using condition numbers\n",
    "􏰅see how Ridge and Lasso regressions are more stable in case of ill-conditioned data 􏰅 \n",
    "learn to manually create design matrices for a survey data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return here to the World Value Survey data, a large survey with ∼90k observations and 430 variables. Your task is to compile a large number of variables into a design matrix. The process easily leads to multicollinearity which you have to detect using condition numbers.\n",
    "Thereafter we over􏰀t, but as this is a fairly large dataset, we only achieve this by selecting a small subsample of the data. You can see that over􏰀tting is often related to large condition numbers, and may lead to linear regression validation RMSE to be ridiculously large. But Ridge and Lasso can handle the situation much better.\n",
    "When reasonably coded, the code should run fast (∼30s), except the stepwise condition number pro- cedure (∼10min). So you may want to be careful and not run that code too often.\n",
    "Please submit a) your code (notebooks, rmd, whatever) and b) the results in a 􏰀nal output form (html or pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# World Values Survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "World Value Survey (WVS) is a large survey, conducted in many countries simultaneously. It revolves around public opinion about traditions, economy, politics, life and other things. It is recommended you to consult the offi􏰄cial questionnaire (uploaded in canvas/􏰀les/wvs).\n",
    "In this task the central question is V23: 􏰁All things considered, how satis􏰀ed are you with your life as a whole these days?􏰂 with answers ranging between 1 (completely dissatis􏰀ed) and 10 (completely satis􏰀ed). We are going to model this variables using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Explore and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load data and take a closer look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import ttest_ind\n",
    "import os\n",
    "from scipy import stats\n",
    "from scipy import *\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/anjalisingh/Desktop/Winter 2020/data\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "print(path)\n",
    "data= pd.read_csv('../data/wvs.csv.bz2', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90350, 328)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)\n",
    "data.shape\n",
    "#data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "1. Browse the WVS documentation and make sure you are familiar with coding of the variable V23. Note: you also have to consult the codebook to understand all the missings and how to remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values 1-10 range from completely dissatisfied to completely satisfied. Values -1 to -5 are inappropriate values and we need ot remove those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Load the data. Remove all the missing observations of V23. I mean all the missings, including the valid numeric codes that denote missing/invalid answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the missing values from V23\n",
    "data = data[data['V23']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now make a table (or a plot) of diff􏰃erent answers. What is the mean satisfaction level on this planet? How large a proportion of people are at 6 or more satis􏰀ed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Count of satisfaction level on this planet')"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plot of V23 and different answers\n",
    "sns.countplot(x=\"V23\", data=data).set_title(\"Count of satisfaction level on this planet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.8290316471911865"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['V23'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The mean satisfaction level on this planet is 6.829"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73.03472168072095"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[data['V23'] >5])/len(data['V23'])*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "73.034% of the people are at 6 or more satisfied"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Create the Design Matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to make the data suitable for a regression model. So far we have either used R-style formulas, or fed data into a ML model directly without much preparatory work. Now it is time to construct the design matrix manually. In case of linear regression, the design matrix is the data matrix that will be directly fed into the formula (X􏰫 · X)−1X􏰫y, or any function that uses this or another similar formula. Design matrix can also be fed directly into other kind of models, such as logistic regression or decision tree. Design matrix is also needed by various libraries, in particular sklearn's LinearRegression, Ridge, and Lasso. There is an example of how to create a design matrix in the lecture notes, Section 2.1.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to add the variables to the design matrix, one-by-one, and each time doing the necessary encoding if appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many variables are categorical. For instance, variable V2, country, is numeric with di􏰨erent numbers representing di􏰨erent countries. So in essence it is a categorical variable where categories are coded as numbers. The same is true for V80, most serious problem in the world. You should convert such variables to dummies (do your still remember pd.get_dummies?) and remove the original variable. But don't forget to remove missings!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of variables contain ordered values instead. For instance, V55 asks how much choice do you feel do you have over your life. The answers range from 1 (no choice at all) to 10 (a great deal of choice). We treat these as numeric response. Although, strictly speaking not correct, the model would be too messy if we were creating a category for each response. However, the missings (-5: inapplicable, -4: not asked etc) are not ordered in any meaningful sense. Hence your task is to remove missings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that many variables, e.g. v74b (important to help people nearby) and v90 (signing petition), contain a very large number of missings, and hence you essentially lose all your data if you include such variables. So you should remove such variables and replace with others that have more valid answers.\n",
    "Was it clear? Good. Enough of talk, let's dive into the real thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create your outcome variable y out of life satisfaction V23 (remove missings!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after removing missing values\n",
    "y = data.V23"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create a design matrix that contains at least 100 variables from the WVS data. Your selected variables should contain at least a few categorical ones, such as V2 country. In each case:\n",
    "(a) remove missing observations\n",
    "(b) convert categorical variable to dummies if appropriate. Don't forget to drop the reference category.\n",
    "This will result in a large amount of code that is essentially copy-paste, but not exactly. It is a little bit tedious to do though. Think about options to make it more automatic but... hint: there are no good options...\n",
    "Note that when converting 100 variables into a design matrix, the latter may end up with many more columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that you end up deleting quite a few observations, everything that contains missings in any of your selected variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>...</th>\n",
       "      <th>MN_228S8</th>\n",
       "      <th>MN_229A</th>\n",
       "      <th>MN_230A</th>\n",
       "      <th>MN_233A</th>\n",
       "      <th>MN_237B1</th>\n",
       "      <th>MN_249A1</th>\n",
       "      <th>MN_249A3</th>\n",
       "      <th>I_RELIGBEL</th>\n",
       "      <th>I_NORM1</th>\n",
       "      <th>I_VOICE1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>89771.000000</td>\n",
       "      <td>85267.000000</td>\n",
       "      <td>88207.000000</td>\n",
       "      <td>85516.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>467.722260</td>\n",
       "      <td>1.095376</td>\n",
       "      <td>1.663566</td>\n",
       "      <td>1.856011</td>\n",
       "      <td>2.548652</td>\n",
       "      <td>1.471945</td>\n",
       "      <td>1.836595</td>\n",
       "      <td>1.834200</td>\n",
       "      <td>2.078578</td>\n",
       "      <td>1.475944</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.354246</td>\n",
       "      <td>-3.447528</td>\n",
       "      <td>-3.578416</td>\n",
       "      <td>-3.514676</td>\n",
       "      <td>-3.820599</td>\n",
       "      <td>-3.513585</td>\n",
       "      <td>-3.521271</td>\n",
       "      <td>0.318095</td>\n",
       "      <td>0.463580</td>\n",
       "      <td>0.333475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>248.409724</td>\n",
       "      <td>0.401337</td>\n",
       "      <td>0.773381</td>\n",
       "      <td>0.908102</td>\n",
       "      <td>1.105639</td>\n",
       "      <td>0.944288</td>\n",
       "      <td>1.111566</td>\n",
       "      <td>0.788411</td>\n",
       "      <td>0.875354</td>\n",
       "      <td>0.505013</td>\n",
       "      <td>...</td>\n",
       "      <td>1.956790</td>\n",
       "      <td>1.660353</td>\n",
       "      <td>1.412735</td>\n",
       "      <td>1.674569</td>\n",
       "      <td>0.711134</td>\n",
       "      <td>1.512512</td>\n",
       "      <td>1.499672</td>\n",
       "      <td>0.465739</td>\n",
       "      <td>0.498675</td>\n",
       "      <td>0.315871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>12.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>276.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>434.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.330000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>702.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>887.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 328 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 V2            V4            V5            V6            V7  \\\n",
       "count  89771.000000  89771.000000  89771.000000  89771.000000  89771.000000   \n",
       "mean     467.722260      1.095376      1.663566      1.856011      2.548652   \n",
       "std      248.409724      0.401337      0.773381      0.908102      1.105639   \n",
       "min       12.000000     -5.000000     -5.000000     -5.000000     -5.000000   \n",
       "25%      276.000000      1.000000      1.000000      1.000000      2.000000   \n",
       "50%      434.000000      1.000000      2.000000      2.000000      3.000000   \n",
       "75%      702.000000      1.000000      2.000000      2.000000      3.000000   \n",
       "max      887.000000      4.000000      4.000000      4.000000      4.000000   \n",
       "\n",
       "                 V8            V9           V10           V11           V12  \\\n",
       "count  89771.000000  89771.000000  89771.000000  89771.000000  89771.000000   \n",
       "mean       1.471945      1.836595      1.834200      2.078578      1.475944   \n",
       "std        0.944288      1.111566      0.788411      0.875354      0.505013   \n",
       "min       -5.000000     -5.000000     -5.000000     -5.000000     -5.000000   \n",
       "25%        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "50%        1.000000      1.000000      2.000000      2.000000      1.000000   \n",
       "75%        2.000000      3.000000      2.000000      3.000000      2.000000   \n",
       "max        4.000000      4.000000      4.000000      4.000000      2.000000   \n",
       "\n",
       "           ...           MN_228S8       MN_229A       MN_230A       MN_233A  \\\n",
       "count      ...       89771.000000  89771.000000  89771.000000  89771.000000   \n",
       "mean       ...          -3.354246     -3.447528     -3.578416     -3.514676   \n",
       "std        ...           1.956790      1.660353      1.412735      1.674569   \n",
       "min        ...          -4.000000     -4.000000     -4.000000     -4.000000   \n",
       "25%        ...          -4.000000     -4.000000     -4.000000     -4.000000   \n",
       "50%        ...          -4.000000     -4.000000     -4.000000     -4.000000   \n",
       "75%        ...          -4.000000     -4.000000     -4.000000     -4.000000   \n",
       "max        ...           4.000000      2.000000      2.000000      5.000000   \n",
       "\n",
       "           MN_237B1      MN_249A1      MN_249A3    I_RELIGBEL       I_NORM1  \\\n",
       "count  89771.000000  89771.000000  89771.000000  85267.000000  88207.000000   \n",
       "mean      -3.820599     -3.513585     -3.521271      0.318095      0.463580   \n",
       "std        0.711134      1.512512      1.499672      0.465739      0.498675   \n",
       "min       -4.000000     -5.000000     -5.000000      0.000000      0.000000   \n",
       "25%       -4.000000     -4.000000     -4.000000      0.000000      0.000000   \n",
       "50%       -4.000000     -4.000000     -4.000000      0.000000      0.000000   \n",
       "75%       -4.000000     -4.000000     -4.000000      1.000000      1.000000   \n",
       "max        1.000000      2.000000      2.000000      1.000000      1.000000   \n",
       "\n",
       "           I_VOICE1  \n",
       "count  85516.000000  \n",
       "mean       0.333475  \n",
       "std        0.315871  \n",
       "min        0.000000  \n",
       "25%        0.000000  \n",
       "50%        0.330000  \n",
       "75%        0.660000  \n",
       "max        1.000000  \n",
       "\n",
       "[8 rows x 328 columns]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to check for the percentage of missing values and removing all the columns where the percentage is greater than 4%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_cols = []\n",
    "for col in list(data.columns):\n",
    "    pct_of_missingness = len(data[data[col] < 0])/len(data)*100\n",
    "    #print(col, len(data[data[col] < 0])/len(data)*100)\n",
    "    if pct_of_missingness < 4:\n",
    "        good_cols.append(col)\n",
    "        #print(col, len(data[data[col] < 0])/len(data)*100)\n",
    "\n",
    "len(good_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the dataframe after removing the columns with percentage of missing values greater than 4.\n",
    "design_frame = data[good_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(89771, 124)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "design_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing the rows that have missing values that are encoded. \n",
    "clean_design = design_frame.replace(to_replace = [-1,-2,-3,-4,-5],value = np.nan)\n",
    "clean_design = clean_design.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45202, 124)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_design.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up with 45202 rows and 124 columnns before creating dummies of the categorical variable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to create dummies for the categorical variables V2,V24,V57,V80 and V83."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_design_dummy = pd.get_dummies(clean_design, columns = ['V2','V24','V57','V80','V83'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45202, 181)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_design_dummy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final dataframe that is going to be converted to the matrix has 45202 rows and 181 columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#creating the final design matrix\n",
    "clean_design_matrix = clean_design_dummy.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Condition numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to compute the condition number of your design matrix. It will quite likely be way too big: some of the answers may be highly correlated, you may forget to drop the reference category, the reference category you drop may have no observations, there may be several variables that contain exactly the same information... But we want to know which variables are the culprits. So instead of computing just a single κ, let's add columns to the design matrix one-by-one, and each time printing the column we added, and the resulting condition number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Compute the condition number of your output matrix in such a manner. The output may look something like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5 1 k =  1.0\n",
      "V6 2 k =  4.784193934313021\n",
      "V7 3 k =  6.784123536579937\n",
      "V8 4 k =  9.831149655129927\n",
      "V9 5 k =  11.050069237303301\n",
      "V10 6 k =  12.482884992971094\n",
      "V11 7 k =  13.958547962327168\n",
      "V12 8 k =  15.139013967900116\n",
      "V13 9 k =  16.302986767838426\n",
      "V14 10 k =  17.295616491564328\n",
      "V15 11 k =  18.05982253705927\n",
      "V16 12 k =  18.92925588520795\n",
      "V17 13 k =  19.533740437449474\n",
      "V18 14 k =  20.201052934956504\n",
      "V19 15 k =  20.82771127619476\n",
      "V20 16 k =  21.438467166499052\n",
      "V21 17 k =  22.123579540076143\n",
      "V22 18 k =  22.706842590654187\n",
      "V23 19 k =  23.366166331584445\n",
      "V25 20 k =  32.38112248428904\n",
      "V26 21 k =  32.461031713620336\n",
      "V27 22 k =  32.51039043499039\n",
      "V28 23 k =  32.528228010499724\n",
      "V30 24 k =  32.538032173478534\n",
      "V31 25 k =  32.57374175662149\n",
      "V32 26 k =  32.58122501883669\n",
      "V33 27 k =  32.58972077142628\n",
      "V34 28 k =  32.74514406957826\n",
      "V37 29 k =  33.07166006347177\n",
      "V39 30 k =  33.578111067005835\n",
      "V41 31 k =  34.04715443361119\n",
      "V43 32 k =  37.54823380504643\n",
      "V44 33 k =  38.177448064950184\n",
      "V45 34 k =  38.846049296465225\n",
      "V47 35 k =  39.54366599791814\n",
      "V48 36 k =  40.23608354039964\n",
      "V49 37 k =  40.63019882976564\n",
      "V52 38 k =  41.03295684754016\n",
      "V55 39 k =  42.361636147962756\n",
      "V56 40 k =  49.735003403975334\n",
      "V58 41 k =  53.90719626900617\n",
      "V59 42 k =  54.285599177034484\n",
      "V60 43 k =  58.445643616083615\n",
      "V62 44 k =  58.74711783621129\n",
      "V64 45 k =  59.1848433390512\n",
      "V67 46 k =  59.553003867522264\n",
      "V68 47 k =  60.081515343577614\n",
      "V70 48 k =  60.27564637855211\n",
      "V71 49 k =  60.999476945817925\n",
      "V72 50 k =  62.38719268122991\n",
      "V73 51 k =  62.92899844372983\n",
      "V75 52 k =  63.915908683579644\n",
      "V76 53 k =  64.69530920523897\n",
      "V77 54 k =  65.9600720914869\n",
      "V78 55 k =  66.56362113107663\n",
      "V79 56 k =  67.13655429651989\n",
      "V82 57 k =  67.76124849838116\n",
      "V84 58 k =  68.08465920784299\n",
      "V96 59 k =  68.69854112655005\n",
      "V98 60 k =  71.46246886898987\n",
      "V99 61 k =  73.34253003931667\n",
      "V100 62 k =  74.57281480463173\n",
      "V102 63 k =  75.98066260310311\n",
      "V103 64 k =  76.10157557452173\n",
      "V104 65 k =  76.48020835586689\n",
      "V105 66 k =  76.83128568945216\n",
      "V108 67 k =  77.59545098910104\n",
      "V110 68 k =  77.95006049842564\n",
      "V111 69 k =  78.48343590303448\n",
      "V113 70 k =  78.9754915616589\n",
      "V114 71 k =  79.43245171472421\n",
      "V115 72 k =  79.88593395054038\n",
      "V131 73 k =  80.39910918193667\n",
      "V133 74 k =  83.37530264167354\n",
      "V134 75 k =  88.120098954167\n",
      "V139 76 k =  91.59370372518397\n",
      "V140 77 k =  95.88992666369857\n",
      "V143 78 k =  100.39369627159532\n",
      "V170 79 k =  100.60042320434638\n",
      "V176 80 k =  100.82036056757782\n",
      "V177 81 k =  101.25138350092219\n",
      "V179 82 k =  101.81359602160781\n",
      "V180 83 k =  103.1515456791293\n",
      "V183 84 k =  104.41213316465813\n",
      "V184 85 k =  104.66677209947237\n",
      "V188 86 k =  104.91776813243433\n",
      "V189 87 k =  105.62666122391617\n",
      "V190 88 k =  106.32213381437703\n",
      "V191 89 k =  107.00519006768347\n",
      "V192 90 k =  107.55629383183766\n",
      "V193 91 k =  110.99459562934764\n",
      "V196 92 k =  114.47191884059379\n",
      "V197 93 k =  115.67349986933782\n",
      "V198 94 k =  118.60145896864586\n",
      "V199 95 k =  119.01024965516224\n",
      "V200 96 k =  119.38493491005104\n",
      "V202 97 k =  119.55787128267266\n",
      "V205 98 k =  119.76579393266273\n",
      "V207 99 k =  120.82558602927958\n",
      "V208 100 k =  121.11263197429373\n",
      "V209 101 k =  121.33486192958715\n",
      "V210 102 k =  121.80425943916977\n",
      "V211 103 k =  122.01765148649018\n",
      "V213 104 k =  122.13686862849553\n",
      "V214 105 k =  122.30110822325535\n",
      "V225 106 k =  122.41552296011197\n",
      "V227 107 k =  122.62809762023025\n",
      "V229 108 k =  122.74914992562789\n",
      "V238 109 k =  123.32267312847215\n",
      "V239 110 k =  123.8897823536754\n",
      "V240 111 k =  125.16248796254685\n",
      "V242 112 k =  125.27885814280734\n",
      "V248 113 k =  195.83927918412226\n",
      "V250 114 k =  196.86683254549362\n",
      "V258A 115 k =  196.96112415510646\n",
      "I_RELIGBEL 116 k =  196.99092662553707\n",
      "I_NORM1 117 k =  197.03561879641316\n",
      "I_VOICE1 118 k =  197.07239780326148\n",
      "V2_31 119 k =  197.1613281242386\n",
      "V2_36 120 k =  457.7719512900687\n",
      "V2_51 121 k =  473.33062882377595\n",
      "V2_76 122 k =  533.1705292770905\n",
      "V2_112 123 k =  533.2943620319002\n",
      "V2_152 124 k =  534.2278114847499\n",
      "V2_156 125 k =  604.0409924822427\n",
      "V2_158 126 k =  604.067563157884\n",
      "V2_170 127 k =  604.529745314694\n",
      "V2_196 128 k =  606.4811623233905\n",
      "V2_233 129 k =  607.6451058029129\n",
      "V2_268 130 k =  609.1814731482738\n",
      "V2_275 131 k =  609.2022604743164\n",
      "V2_276 132 k =  609.3825482076187\n",
      "V2_288 133 k =  613.5317109865217\n",
      "V2_356 134 k =  613.5323025879436\n",
      "V2_368 135 k =  614.115070294389\n",
      "V2_392 136 k =  614.4629355396514\n",
      "V2_398 137 k =  683.8964800414328\n",
      "V2_400 138 k =  686.8448146957968\n",
      "V2_410 139 k =  686.89984986136\n",
      "V2_417 140 k =  694.3030264408695\n",
      "V2_422 141 k =  697.3385438812138\n",
      "V2_434 142 k =  700.0601414208904\n",
      "V2_458 143 k =  703.273021660089\n",
      "V2_484 144 k =  711.0015151939317\n",
      "V2_504 145 k =  714.4363222582398\n",
      "V2_528 146 k =  738.4417279655617\n",
      "V2_566 147 k =  745.4532893895092\n",
      "V2_586 148 k =  746.6632845262685\n",
      "V2_604 149 k =  753.3637157194503\n",
      "V2_608 150 k =  761.2631606735106\n",
      "V2_616 151 k =  770.2375952141501\n",
      "V2_642 152 k =  782.4099028826389\n",
      "V2_643 153 k =  802.1666146609277\n",
      "V2_646 154 k =  818.5678358716067\n",
      "V2_702 155 k =  831.9223429626884\n",
      "V2_705 156 k =  858.2978417931911\n",
      "V2_710 157 k =  882.942071429219\n",
      "V2_716 158 k =  926.6750020117361\n",
      "V2_724 159 k =  992.0650648647669\n",
      "V2_752 160 k =  1021.9103893875143\n",
      "V2_764 161 k =  1068.5597481275786\n",
      "V2_780 162 k =  1125.9470023058775\n",
      "V2_788 163 k =  1189.2199965632963\n",
      "V2_792 164 k =  1248.7089888854266\n",
      "V2_804 165 k =  1373.3059426688894\n",
      "V2_840 166 k =  1573.6526892829615\n",
      "V2_858 167 k =  2194.594439640524\n",
      "V2_860 168 k =  2529.954545731054\n",
      "V2_887 169 k =  3409.8419413171896\n",
      "V24_2.0 170 k =  4356.397933350065\n",
      "V57_2.0 171 k =  4357.135623885868\n",
      "V57_3.0 172 k =  4357.893423220405\n",
      "V57_4.0 173 k =  4357.895747108093\n",
      "V57_5.0 174 k =  4357.976477033222\n",
      "V57_6.0 175 k =  4358.153735075952\n",
      "V80_2.0 176 k =  4360.825380330925\n",
      "V80_3.0 177 k =  4360.835883372873\n",
      "V80_4.0 178 k =  4361.494671727687\n",
      "V80_5.0 179 k =  4361.530583564057\n",
      "V83_2.0 180 k =  4361.543787173197\n"
     ]
    }
   ],
   "source": [
    "list_of_cols = list(clean_design_dummy.columns)\n",
    "for num_of_columns in range(1, len(list_of_cols)):\n",
    "    df = clean_design_dummy[list_of_cols[:num_of_columns]]\n",
    "    df_matrix = df.as_matrix()\n",
    "    k = np.linalg.cond(df_matrix)\n",
    "    print(list_of_cols[num_of_columns], num_of_columns, \"k = \", k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. if the condition number turns out too big (say, over 100,000 or so), identify the culprit, and 􏰩x it in the design matrix above. In some case you may remove the variable and replace by another one, you may also consider merging small categories into a larger one or something else. You may go back-and forth quite a few times before you get a suitably well-conditioned design matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case the condition number isn't very high, but stops at around 4k. We do not play around with this any further, and can call this a well-conditional design matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  4 Do Some Social Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting further, let's do a simple social science analysis. How is life satisfaction related to health (v11), perceived control over life (v55) and fi􏰩nancial situation (v59)? Let's analyze association between satisfaction and just these three variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. run a linear regression models explaining satisfaction with these three variables. Present the output table.\n",
    "I recommend to use statsmodels.formula.api for this task (but you have to use sklearn later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept    4.274996\n",
      "V11         -0.418695\n",
      "V55          0.210888\n",
      "V59          0.331759\n",
      "dtype: float64\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    V23   R-squared:                       0.294\n",
      "Model:                            OLS   Adj. R-squared:                  0.294\n",
      "Method:                 Least Squares   F-statistic:                 1.244e+04\n",
      "Date:                Sat, 07 Mar 2020   Prob (F-statistic):               0.00\n",
      "Time:                        13:12:49   Log-Likelihood:            -1.8553e+05\n",
      "No. Observations:               89771   AIC:                         3.711e+05\n",
      "Df Residuals:                   89767   BIC:                         3.711e+05\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept      4.2750      0.030    140.927      0.000       4.216       4.334\n",
      "V11           -0.4187      0.008    -55.660      0.000      -0.433      -0.404\n",
      "V55            0.2109      0.003     75.857      0.000       0.205       0.216\n",
      "V59            0.3318      0.003    123.436      0.000       0.326       0.337\n",
      "==============================================================================\n",
      "Omnibus:                     3194.982   Durbin-Watson:                   1.674\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             5007.961\n",
      "Skew:                          -0.336   Prob(JB):                         0.00\n",
      "Kurtosis:                       3.942   Cond. No.                         47.3\n",
      "==============================================================================\n",
      "\n",
      "Warnings:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "#linear regression model with v11,v55,v59\n",
    "linear_output = smf.ols(formula = 'V23 ~ V11 + V55 + V59', data = data).fit()\n",
    "print(linear_output.params)\n",
    "print(linear_output.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. comment the output table in terms of relative e􏰨ffect size and statistical signi􏰩cance. Any surprises for you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) All the three variables V11,V55, V59 are satistically significant as their p-value is 0. <br> \n",
    "2) The value of adjusted R square is extremely less (i.e 0.294), indicating that it is not a good fit. <br>\n",
    "3) Also, if we look at the intercepts, the satisfaction of life depends on perceived control of life and financial situation positively. <br>\n",
    "4) Satisfaction of life increases with decreses in the state of health. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increases in the satisfaction of life with a decrease in the state of health is extremely surprising. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compute and present RMSE (just on training data). This will serve as the benchmark for the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "ypred= linear_output.predict(data)\n",
    "#ypred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.911297527702806"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse =  np.sqrt(((ypred - data['V23']) ** 2).mean())\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark rmse is 1.91129"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Back to ML: Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to use all these variables to model satisfaction. Use sklearn.linear_model.LinearRegression here as this is easy to be switched with ridge and lasso, and it takes in the design matrix directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. compute the condition number for your design matrix (just a single number, not the stepwise pro- cedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4363.188119591444"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.cond(clean_design_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition matrix of the design matrix is 4363.188"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Split the data into training-validation chunks (80-20 or so)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "X= clean_design_dummy.drop(['V23'],axis=1)\n",
    "y = clean_design_dummy.V23\n",
    "\n",
    "#creating the training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X.as_matrix(), y.as_matrix(), test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. compute the condition number for your training design matrix (just a single number, not the stepwise procedure)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4334.285503718183"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "condition_number_training = np.linalg.cond(X_train)\n",
    "condition_number_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition number of the training design matrix is 4267.611"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 􏰩Fit a linear regression model where you describe satisfaction with the design matrix X you just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. predict and compute RMSE on training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training data 1.6799990710356105\n"
     ]
    }
   ],
   "source": [
    "#trainign RMSE\n",
    "y_pred_train= lm.predict(X_train)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "print(\"RMSE for training data\",RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. predict and compute RMSE on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for testing data 1.67343385762339\n"
     ]
    }
   ],
   "source": [
    "#testing RMSE\n",
    "y_pred_test= lm.predict(X_val)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(y_val, y_pred_test))\n",
    "print(\"RMSE for testing data\", RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. repeat the previous with Ridge regression, play a little with di􏰨erent α-s. Which α gave you the best testing RMSE? (No need for a rigorous analysis, just play a little)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE when alpha for training data is  0.001 : 1.6799990710501704\n",
      "RMSE when alpha for testing data is  0.001 : 1.6734337883178538 \n",
      "\n",
      "RMSE when alpha for training data is  0.005 : 1.6799990713990278\n",
      "RMSE when alpha for testing data is  0.005 : 1.6734335115869898 \n",
      "\n",
      "RMSE when alpha for training data is  0.1 : 1.67999921104602\n",
      "RMSE when alpha for testing data is  0.1 : 1.6734271630133428 \n",
      "\n",
      "RMSE when alpha for training data is  0.5 : 1.6800020826737294\n",
      "RMSE when alpha for testing data is  0.5 : 1.6734044769536454 \n",
      "\n",
      "RMSE when alpha for training data is  1 : 1.6800092100631168\n",
      "RMSE when alpha for testing data is  1 : 1.673382969503913 \n",
      "\n",
      "RMSE when alpha for training data is  2 : 1.6800290310893082\n",
      "RMSE when alpha for testing data is  2 : 1.6733534739194327 \n",
      "\n",
      "RMSE when alpha for training data is  5 : 1.6800935259572412\n",
      "RMSE when alpha for testing data is  5 : 1.6733046638531135 \n",
      "\n",
      "RMSE when alpha for training data is  10 : 1.6801800648195266\n",
      "RMSE when alpha for testing data is  10 : 1.6732493987670924 \n",
      "\n",
      "RMSE when alpha for training data is  100 : 1.6817347293453537\n",
      "RMSE when alpha for testing data is  100 : 1.6733859715778203 \n",
      "\n",
      "RMSE when alpha for training data is  500 : 1.689785574963557\n",
      "RMSE when alpha for testing data is  500 : 1.6790077368777254 \n",
      "\n",
      "RMSE when alpha for training data is  1000 : 1.6963826444093324\n",
      "RMSE when alpha for testing data is  1000 : 1.6841979989851987 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#list of alpha values\n",
    "alpha_value = [0.001, 0.005, 0.1, 0.5, 1,2,5,10,100,500,1000]\n",
    "for i in alpha_value:\n",
    "    rr = Ridge(alpha = i)\n",
    "    ridge_model = rr.fit(X_train, y_train)\n",
    "    y_ridge_train =rr.predict(X_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_ridge_train))\n",
    "    print(\"RMSE when alpha for training data is \",i,\":\",RMSE)\n",
    "    y_ridge_val= rr.predict(X_val)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_val, y_ridge_val))\n",
    "    print(\"RMSE when alpha for testing data is \",i,\":\",RMSE, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha value for ridge regression does not cause drastic changes to the RMSE values, however, as the value of alpha increases the RMSE increases extremely slightly. \n",
    "The alpha value of 0.001 among the select alpha values gives the best testing RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. and repeat with Lasso regression again playing a little with di􏰨erent α-s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE when alpha for training data is  0.001 : 1.6819836087996627\n",
      "RMSE when alpha for testing data is  0.001 : 1.674606100225012 \n",
      "\n",
      "RMSE when alpha for training data is  0.005 : 1.6978448759231217\n",
      "RMSE when alpha for testing data is  0.005 : 1.687224405444071 \n",
      "\n",
      "RMSE when alpha for training data is  0.1 : 1.7497608931252808\n",
      "RMSE when alpha for testing data is  0.1 : 1.734956890036397 \n",
      "\n",
      "RMSE when alpha for training data is  0.5 : 1.9071995107491344\n",
      "RMSE when alpha for testing data is  0.5 : 1.8782427412323115 \n",
      "\n",
      "RMSE when alpha for training data is  1 : 1.9671439171266552\n",
      "RMSE when alpha for testing data is  1 : 1.9349918887716278 \n",
      "\n",
      "RMSE when alpha for training data is  2 : 2.141054150744172\n",
      "RMSE when alpha for testing data is  2 : 2.105460834758499 \n",
      "\n",
      "RMSE when alpha for training data is  5 : 2.2290794661260294\n",
      "RMSE when alpha for testing data is  5 : 2.194896473809558 \n",
      "\n",
      "RMSE when alpha for training data is  10 : 2.2290794661260294\n",
      "RMSE when alpha for testing data is  10 : 2.194896473809558 \n",
      "\n",
      "RMSE when alpha for training data is  100 : 2.2290794661260294\n",
      "RMSE when alpha for testing data is  100 : 2.194896473809558 \n",
      "\n",
      "RMSE when alpha for training data is  500 : 2.2290794661260294\n",
      "RMSE when alpha for testing data is  500 : 2.194896473809558 \n",
      "\n",
      "RMSE when alpha for training data is  1000 : 2.2290794661260294\n",
      "RMSE when alpha for testing data is  1000 : 2.194896473809558 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha_value = [0.001, 0.005, 0.1, 0.5, 1,2,5,10,100,500,1000]\n",
    "for i in alpha_value:\n",
    "    ll = Lasso(alpha = i)\n",
    "    lasso_model = ll.fit(X_train, y_train)\n",
    "    y_lasso_train = ll.predict(X_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_train, y_lasso_train))\n",
    "    print(\"RMSE when alpha for training data is \",i,\":\",RMSE)\n",
    "    y_lasso_val= ll.predict(X_val)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(y_val, y_lasso_val))\n",
    "    print(\"RMSE when alpha for testing data is \",i,\":\",RMSE, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alpha value for lasso regression increases the RMSE as the value of alpha increases. When the value of alpha is 0.001, the RMSE is 1.685, and when the value of alpha is 1000, the rmse is 2.2211"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. comment your results:\n",
    "(a) compare RMSE on testing/training data. What does this suggest in terms of over􏰩fitting? \n",
    "(b) compare RMSE for OLS, Ridge and Lasso\n",
    "(c) compare the resulting RMSE with the small benchmark model you did above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your results are like mine, you see that a) RMSE on both testing-training sets are similar; b) RMSE for OLS, Ridge, Lasso are similar; and c) all these 100 or so extra variables add very little explanatory power to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Compare RMSE on testing/training data \n",
    "\n",
    "The RMSE on both testing/training sets are similar with a value of 1.6838 for test dataset vs 1.6772 for training dataset.\n",
    "This means that it is not overfitting and the model has very low variance. \n",
    "\n",
    "b) compare RMSE for OLS, Ridge and Lasso\n",
    "The RMSE for OLS, Ridge and Lasso are similar, with almost no differences seen when the right alpha values are chosen for ridge and lasso regression\n",
    "\n",
    "c)The resulting RMSE (for OLS, Ridge and Lasso(1.68) ) is slighly less than the RMSE for the benchmark model (1.911). However, they are extremely close, and the extra variables in the bechmark model provide very little explanation to the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Let's Overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As WVS is a relatively large dataset we cannot easily over􏰩t by adding more variables. But we can go another easy route instead: we take a subsample.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a subsample of your design matrix and the outcome variable. Choose a large-ish sample that over􏰩ts.\n",
    "The size depends on which variables do you exactly choose, in my case 2000 obs rarely over􏰩ts (it depends on the train-validation split), 1000 typically over􏰩ts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 181)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#taking 10% of the value of the original dataset\n",
    "subset_set = clean_design_dummy.sample(frac=0.0125, replace=False, random_state=4500)\n",
    "subset_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. repeat the steps you did above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition matrix for subset of the design matrix 6486.855417001884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "print(\"condition matrix for subset of the design matrix\",np.linalg.cond(subset_set.as_matrix()))\n",
    "#splitting the data into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(565, 180)\n",
      "(565,)\n",
      "RMSE for training data 1.3025787127056754 \n",
      "\n",
      "RMSE for testing data 2.0202694235625445 \n",
      "\n",
      "Ridge Regression \n",
      "\n",
      "RMSE when alpha for training data is  0.001 : 1.3025859232698425\n",
      "RMSE when alpha for testing data is  0.001 : 2.020282193371057 \n",
      "\n",
      "RMSE when alpha for training data is  0.005 : 1.3027132588290375\n",
      "RMSE when alpha for testing data is  0.005 : 2.020288593493901 \n",
      "\n",
      "RMSE when alpha for training data is  0.1 : 1.30577894712611\n",
      "RMSE when alpha for testing data is  0.1 : 2.011082722201325 \n",
      "\n",
      "RMSE when alpha for training data is  0.5 : 1.3105047585328076\n",
      "RMSE when alpha for testing data is  0.5 : 1.968937375904311 \n",
      "\n",
      "RMSE when alpha for training data is  1 : 1.317101366633519\n",
      "RMSE when alpha for testing data is  1 : 1.9326601562314305 \n",
      "\n",
      "RMSE when alpha for training data is  2 : 1.3302493108399318\n",
      "RMSE when alpha for testing data is  2 : 1.8868120149955177 \n",
      "\n",
      "RMSE when alpha for training data is  5 : 1.360234887249043\n",
      "RMSE when alpha for testing data is  5 : 1.8214517141713407 \n",
      "\n",
      "RMSE when alpha for training data is  10 : 1.3901412355462375\n",
      "RMSE when alpha for testing data is  10 : 1.7747329612304452 \n",
      "\n",
      "RMSE when alpha for training data is  100 : 1.499299179491466\n",
      "RMSE when alpha for testing data is  100 : 1.6528880496144056 \n",
      "\n",
      "RMSE when alpha for training data is  500 : 1.6022175354504449\n",
      "RMSE when alpha for testing data is  500 : 1.6247223468641099 \n",
      "\n",
      "RMSE when alpha for training data is  1000 : 1.6612982381330348\n",
      "RMSE when alpha for testing data is  1000 : 1.6411311692762305 \n",
      "\n",
      "Lasso Regression \n",
      "\n",
      "RMSE when alpha for training data is  0.001 : 1.3101253125977903\n",
      "RMSE when alpha for testing data is  0.001 : 1.9517237185476393 \n",
      "\n",
      "RMSE when alpha for training data is  0.005 : 1.3586709042981768\n",
      "RMSE when alpha for testing data is  0.005 : 1.7994150732314853 \n",
      "\n",
      "RMSE when alpha for training data is  0.1 : 1.6433673437953822\n",
      "RMSE when alpha for testing data is  0.1 : 1.5952190338235916 \n",
      "\n",
      "RMSE when alpha for training data is  0.5 : 1.834095807388825\n",
      "RMSE when alpha for testing data is  0.5 : 1.7064874771763223 \n",
      "\n",
      "RMSE when alpha for training data is  1 : 1.9057870200885947\n",
      "RMSE when alpha for testing data is  1 : 1.7534145431071437 \n",
      "\n",
      "RMSE when alpha for training data is  2 : 2.0660286701230195\n",
      "RMSE when alpha for testing data is  2 : 1.8986194974068218 \n",
      "\n",
      "RMSE when alpha for training data is  5 : 2.211578391181967\n",
      "RMSE when alpha for testing data is  5 : 2.0354956209866524 \n",
      "\n",
      "RMSE when alpha for training data is  10 : 2.211578391181967\n",
      "RMSE when alpha for testing data is  10 : 2.0354956209866524 \n",
      "\n",
      "RMSE when alpha for training data is  100 : 2.211578391181967\n",
      "RMSE when alpha for testing data is  100 : 2.0354956209866524 \n",
      "\n",
      "RMSE when alpha for training data is  500 : 2.211578391181967\n",
      "RMSE when alpha for testing data is  500 : 2.0354956209866524 \n",
      "\n",
      "RMSE when alpha for training data is  1000 : 2.211578391181967\n",
      "RMSE when alpha for testing data is  1000 : 2.0354956209866524 \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  import sys\n"
     ]
    }
   ],
   "source": [
    "X_subset= subset_set.drop(['V23'],axis=1)\n",
    "y_subset = subset_set.V23\n",
    "print(X_subset.shape)\n",
    "print(y_subset.shape)\n",
    "\n",
    "#creating the training and validation sets\n",
    "XS_train, XS_val, yS_train, yS_val = train_test_split(X_subset.as_matrix(), y_subset.as_matrix(), test_size=0.2)\n",
    "\n",
    "\n",
    "#linear regression\n",
    "lm = linear_model.LinearRegression()\n",
    "model = lm.fit(XS_train, yS_train)\n",
    "yS_pred_train= lm.predict(XS_train)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(yS_train, yS_pred_train))\n",
    "print(\"RMSE for training data\",RMSE,\"\\n\")\n",
    "yS_pred_test= lm.predict(XS_val)\n",
    "RMSE = np.sqrt(metrics.mean_squared_error(yS_val, yS_pred_test))\n",
    "print(\"RMSE for testing data\", RMSE,\"\\n\")\n",
    "\n",
    "\n",
    "#ridge\n",
    "alpha_value = [0.001, 0.005, 0.1, 0.5, 1,2,5,10,100,500,1000]\n",
    "print(\"Ridge Regression\", \"\\n\")\n",
    "for i in alpha_value:\n",
    "    rr = Ridge(alpha = i)\n",
    "    ridge_model = rr.fit(XS_train, yS_train)\n",
    "    yS_ridge_train= rr.predict(XS_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(yS_train, yS_ridge_train))\n",
    "    print(\"RMSE when alpha for training data is \",i,\":\",RMSE)\n",
    "    yS_ridge_val= rr.predict(XS_val)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(yS_val, yS_ridge_val))\n",
    "    print(\"RMSE when alpha for testing data is \",i,\":\",RMSE, \"\\n\")\n",
    "    \n",
    "    \n",
    "#lasso\n",
    "print(\"Lasso Regression\", \"\\n\")\n",
    "alpha_value = [0.001, 0.005, 0.1, 0.5, 1,2,5,10,100,500,1000]\n",
    "for i in alpha_value:\n",
    "    ll = Lasso(alpha = i)\n",
    "    lasso_model = ll.fit(XS_train, yS_train)\n",
    "    yS_lasso_train= ll.predict(XS_train)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(yS_train, yS_lasso_train))\n",
    "    print(\"RMSE when alpha for training data is \",i,\":\",RMSE)\n",
    "    yS_lasso_val= ll.predict(XS_val)\n",
    "    RMSE = np.sqrt(metrics.mean_squared_error(yS_val, yS_lasso_val))\n",
    "    print(\"RMSE when alpha for testing data is \",i,\":\",RMSE, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. comment how do OLS, Ridge, Lasso perform on testing/training in case of over􏰩tting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our case of overfitting, our subset dataset consists of 565 rows. The RMSE for test data is higher when compared to the train data for a simple/ordinary linear regression model, i.e. we see a marked difference between the test and train RMSEs. \n",
    "However, when models being used are lasso and ridge regression, for certain values of alpha, the test and train RMSE are extremely close to each other.\n",
    "For example, when using Ridge regression, the test RMSE is very close to the train RMSE for the alpha value of 1000. Similarly, for the alpha value of 2, the test and train RMSE are very close to each other in the case of Lasso regression. \n",
    "\n",
    "To summarize, we can say that Lasso and Ridge regression perform better than linear regression in case of overfitting, and that these fitted models are much less prone to variance than OLS (Ordinary Least Squares) regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. comment the condition number of design matrix and overfi􏰩tting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The condition number of the subset design matrix is always higher than the whole design matrix. That means that the datasets which are prone to overfitting have a higher condition matrix and the OLS(ordinary least square) regression will be less effective than lasso and ridge regression.\n",
    "The condition number can be used as an identified to evaluate overfitting in models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
